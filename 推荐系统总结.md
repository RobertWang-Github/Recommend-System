简单来说,推荐是个分类问题,待推荐的对象item和人person之间构成特征二元组(item_i,person_i),而click与否则是标签(c_i).  
然而,如果考虑到click和order的差异,我们需要最大化  
<a href="https://www.codecogs.com/eqnedit.php?latex=income&space;=&space;P(click|impression)*P(order|click)*price" target="_blank"><img src="https://latex.codecogs.com/gif.latex?income&space;=&space;P(click|impression)*P(order|click)*price" title="income = P(click|impression)*P(order|click)*price" /></a>  
impression(曝光)->click(点击)->order(实现) * price(价格) 才是我们需要的收益.

下面对几个的推荐系统作总结:  


1.FM(Factorization Machines 因子分解机)  
&emsp;&emsp;1.线性部分(将各特征线性加权,显然线性部分直接用sigmoid激活就是LR)  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_linear&space;=&space;\sigma(<\overrightarrow{w},\overrightarrow{x}>))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_linear&space;=&space;<\overrightarrow{w},\overrightarrow{x}>" title="y_linear = <\overrightarrow{w},\overrightarrow{x}>" /></a>  
&emsp;&emsp;2.二阶部分(各特征两两组合(此处以乘积形式))  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" /></a>  
&emsp;&emsp;&emsp;&emsp;<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>是一个n阶对称阵,因为任何两个特征x_i和x_j的关系是对称的.  
&emsp;&emsp;&emsp;&emsp;那么,<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>可以分解为W^T * W.W为(n,k)二阶张量.原函数形式就变成了  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=W*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W*\overrightarrow{x}" title="W*\overrightarrow{x}" /></a>可以看做是将向量x映射到低维(k维度)空间,因为W的每个行向量此时为一个基.如此,如果x是个高维稀疏向量,就被映射到了低维空间,此时再计算两两关系就会节省计算量.显然,这个低维空间是被训练的,以拿到最适合的向量表示,即embedding.向量x经过一个放射变换成为了另一个向量.这个放射变换的参数是训练参数.
&emsp;&emsp;&emsp;&emsp;计算上来说,W*\overrightarrow{x}可以看作W的列向量按照x的分量线性加权.那么原式计算可以优化为  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" title="y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" /></a>  
&emsp;&emsp;&emsp;&emsp;其中,v_i和v_j皆为W的列向量.  

&emsp;&emsp;3.输出部分(采用sigmoid作为激活函数)  
&emsp;&emsp;&emsp;&emsp;  将线性部分和二阶部分直接相加,再用激活函数作用即可.  

&emsp;&emsp;4.衍生方案(多领域FM求和)(还是注重二阶特征组合)  
&emsp;&emsp;&emsp;&emsp; FM中只有一个领域存在时,向量x的嵌入向量W * x自己和自己作內积,实现二阶组合.但是,当存在不同领域field时,可以采取将x切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding两两作內积,再将所有內积结果和线性部分结果求和送入激活函数.  

&emsp;&emsp;5.衍生方案(多领域FM拼接.embedding+MLP)(能体现特征高阶组合)
&emsp;&emsp;&emsp;&emsp;  对二阶部分的处理还可以这样做:将向量x按领域切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding拼接(concat)成一个向量,送入MLP(多层感知器,即激活函数(仿射变换)->激活函数(仿射变换)->...多个串联),这样最终得到一个向量,执行一个线性加权和一个sigmoid激活就拿到了最终结果.  
&emsp;&emsp;&emsp;&emsp; embedding+MLP的落地方案有很多.FNN就是其中一个,用FM预训练这L个放射变换(FM更浅且有线性部分),且嵌入后各领域维度相同.另外,FNN似乎没有和FM一样的线性部分,所以对低阶特征组合的考虑不足,仅体现在预训练阶段.


2.FFM  
&emsp;&emsp;



3.DeepFM  
&emsp;&emsp;特点1: 不需要预训练  
&emsp;&emsp;特点2: 有FM部分  
&emsp;&emsp;特点3: 有MLP部分  
&emsp;&emsp;结构：wide(领域足够多) deep(MLP足够深) -> 一阶,二阶,高阶特征都有体现.  
&emsp;&emsp;sigmoid(y_linear(不区分领域直接加权求和) + y_2d-polynomial(多领域分别embedding,两两作內积,求和) + y_MLP(多领域分别embedding,拼接后送入MLP))  

4.NFM(神经因子分解机)  
&emsp;&emsp;特点1:主要是对FM中二阶项的产生方式进行了改进.FM中的二阶项直接由不同领域的Embedding两两做內积得到.NFM修改为不同领域的Embedding两两作Hadamard product(向量对应元素相乘,得到的是一个向量),再将得到的向量求和(仍然是一个向量,长度和embedding保持一致)送入MLP,最后经过线性加权进入激活函数.将Embedding求內积的过程拆成了作Hadamard product和求和,并推迟了求和的过程.  
&emsp;&emsp;特点2:当MLP中的仿射变换和激活函数为恒等变换时,NFM退化成FM.NFM仍然可以加上y_linear部分.  

5.









0.标签匹配(此时就不得不介绍下本人的原创方法)  
&emsp;&emsp;1.给item打上标签,此时每个类(这个类似乎就是算法中通常说到的field领域)下的标签都会有一个经验分布(由频数或者分数归一化得到).具体哪些标签算作一类要由规则确定.假如是类为行业关键词,那么这些词就构成了这个类下的标签.
&emsp;&emsp;2.搜集用户person在item上的信息(即将每个用户在过去某个时间段内浏览的item的标签分类汇总,拿到每个类下标签的经验分布)  
&emsp;&emsp;3.依据person的各类下的概率分布和item的各类下的概率分布,按item标签的来源基于不同权重,各类予以不同权重,类内采用命中标签的概率值相乘的方式获取分数,再将该分数和类与来源的权重层层加权,最后将总分数排序,作为推荐候选名单.  
&emsp;&emsp;4.也可以将person和item的概率分布计算交叉熵作为分数进行加权.  
&emsp;&emsp;5.也可以将person和item的概率分布作为特征传入分类算法.  





