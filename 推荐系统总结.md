简单来说,推荐是个分类问题,待推荐的对象item和人person之间构成特征二元组(item_i,person_i),而click与否则是标签(c_i).  
然而,如果考虑到click和order的差异,我们需要最大化  
<a href="https://www.codecogs.com/eqnedit.php?latex=income&space;=&space;P(click|impression)*P(order|click)*price" target="_blank"><img src="https://latex.codecogs.com/gif.latex?income&space;=&space;P(click|impression)*P(order|click)*price" title="income = P(click|impression)*P(order|click)*price" /></a>  
impression(曝光)->click(点击)->order(实现) * price(价格) 才是我们需要的收益.  
另外,值得注意的一点是,推荐系统的可能存在对算力的一定要求.这使得我们没法直接对所有的对象item和人person执行推荐计算.只能从所有的item中先筛选出一批候选item,有时甚至要把person也筛掉一部分.这样不仅使得算力和存储占用更低,也使得结果可能更好.这就是有人说到的检索(retrieval) 的方法,对大数据集进行初步筛选,返回最匹配query的一部分物品列表.这里的检索通常会结合采用机器学习模型(machine-learned models)和人工定义规则(human-defined rules)两种方法。从大规模样本中召回最佳候选集之后,再使用排序系统对每个物品进行算分、排序,分数P(y|x),y 是用户采取的行动(比如说参加会议行为),x是特征.      


下面对几个的推荐系统作总结:  


1.FM(Factorization Machines 因子分解机)  
&emsp;&emsp;1.线性部分(将各特征线性加权,显然线性部分直接用sigmoid激活就是LR)  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_linear&space;=&space;\sigma(<\overrightarrow{w},\overrightarrow{x}>))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_linear&space;=&space;<\overrightarrow{w},\overrightarrow{x}>" title="y_linear = <\overrightarrow{w},\overrightarrow{x}>" /></a>  
&emsp;&emsp;2.二阶部分(各特征两两组合(此处以乘积形式))  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" /></a>  
&emsp;&emsp;&emsp;&emsp;<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>是一个n阶对称阵,因为任何两个特征x_i和x_j的关系是对称的.  
&emsp;&emsp;&emsp;&emsp;那么,<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>可以分解为W^T * W.W为(n,k)二阶张量.原函数形式就变成了  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=W*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W*\overrightarrow{x}" title="W*\overrightarrow{x}" /></a>可以看做是将向量x映射到低维(k维度)空间,因为W的每个行向量此时为一个基.如此,如果x是个高维稀疏向量,就被映射到了低维空间,此时再计算两两关系就会节省计算量.显然,这个低维空间是被训练的,以拿到最适合的向量表示,即embedding.向量x经过一个放射变换成为了另一个向量.这个放射变换的参数是训练参数.  
&emsp;&emsp;&emsp;&emsp;计算上来说,W*\overrightarrow{x}可以看作W的列向量按照x的分量线性加权.那么原式计算可以优化为  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" title="y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" /></a>  
&emsp;&emsp;&emsp;&emsp;其中,v_i和v_j皆为W的列向量.  这个优化方式只要涉及到embedding就可以使用.

&emsp;&emsp;3.输出部分(采用sigmoid作为激活函数)  
&emsp;&emsp;&emsp;&emsp;  将线性部分和二阶部分直接相加,再用激活函数作用即可.  

&emsp;&emsp;4.衍生方案(多领域FM求和)(还是注重二阶特征组合)  
&emsp;&emsp;&emsp;&emsp; FM中只有一个领域存在时,向量x的嵌入向量W * x自己和自己作內积,实现二阶组合.但是,当存在不同领域field时,可以采取将x切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding两两作內积,再将所有內积结果和线性部分结果求和送入激活函数.  

&emsp;&emsp;5.衍生方案(多领域FM拼接.embedding+MLP)(能体现特征高阶组合)
&emsp;&emsp;&emsp;&emsp;  对二阶部分的处理还可以这样做:将向量x按领域切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding拼接(concat)成一个向量,送入MLP(多层感知器,即激活函数(仿射变换)->激活函数(仿射变换)->...多个串联),这样最终得到一个向量,执行一个线性加权和一个sigmoid激活就拿到了最终结果.  
&emsp;&emsp;&emsp;&emsp; embedding+MLP的落地方案有很多.FNN就是其中一个,用FM预训练这L个放射变换(FM更浅且有线性部分),且嵌入后各领域维度相同.另外,FNN似乎没有和FM一样的线性部分,所以对低阶特征组合的考虑不足,仅体现在预训练阶段.


2.FFM  
&emsp;&emsp;就是FM中的衍生方案4.在FM中引入field概念.



3.DeepFM  
&emsp;&emsp;特点1: 不需要预训练  
&emsp;&emsp;特点2: 有FM部分  
&emsp;&emsp;特点3: 有MLP部分  
&emsp;&emsp;结构：wide(领域足够多) deep(MLP足够深) -> 一阶,二阶,高阶特征都有体现.  
&emsp;&emsp;sigmoid(y_linear(不区分领域直接加权求和) + y_2d-polynomial(多领域分别embedding,两两作內积,求和) + y_MLP(多领域分别embedding,拼接后送入MLP))  

4.NFM(神经因子分解机)  
&emsp;&emsp;特点1:主要是对FM中二阶项的产生方式进行了改进.FM中的二阶项直接由不同领域的Embedding两两做內积得到.NFM修改为不同领域的Embedding两两作Hadamard product(向量对应元素相乘,得到的是一个向量),再将得到的向量求和(仍然是一个向量,长度和embedding保持一致)送入MLP,最后经过线性加权进入激活函数.将Embedding求內积的过程拆成了作Hadamard product和求和,并推迟了求和的过程.  
&emsp;&emsp;特点2:当MLP中的仿射变换和激活函数为恒等变换时,NFM退化成FM.NFM仍然可以加上y_linear部分.  

5.AFM(Attentional Factorization Machine 注意力FM)  
&emsp;&emsp;和NFM类似,都是为了对FM中二阶项进行改进.和NFM不同之处有2点.  
&emsp;&emsp;特点1:与NFM相比,没有MLP的部分,直接线性加权求和送入sigmoid
&emsp;&emsp;特点2:与NFM相比,在不同领域(设共G个)Embedding两两作Hadamard product之后(得到a_ij表示embedding i和j作Hadamard product),并不是马上将这些向量求和,而是将a_ij送入一个Attention Net, Attention Net就是 将a_ij作一个仿射变换+bias送入ReLu,在将各分量线性加权(权重系数为h)得到一个标量.C_G^2个这样的标量送入softmax(exp(a_ij)/sum(exp(a_ij))),拿到每个a_ij的权重w_ij,再计算sum(a_ij * w_ij)对所有的i,j.简而言之,就是在NFM向量求和部分加上权重了.  

6.PNN(Product-based Neural Network 基于乘积的NN)  
&emsp;&emsp;  有两种形态,內积(IPNN)与外积(OPNN).这两个形态主要是针对y_nonlinear的.不妨设field数为G
&emsp;&emsp;  內积形态:那么不同的field(将向量x切片成G份,再通过仿射变换)会得到不同的embedding共计G个,两两做內积共有G * G个元素a_ij,作为矩阵A的元素.再取D1个不同的矩阵和A分别作对应元素相乘再求和,此时得到D1个向量(因为每次求和后都是一个标量,共有D1个).这个向量就是y_nonlinear.
&emsp;&emsp;  外积形态: 此时a_ij不是由內积得到的标量,而是由embedding_i和embedding_j作 <embedding_i,embedding_j^T>得到的是一个矩阵.A由所有a_ij求和得到.其余部分一致. 
&emsp;&emsp;  此处y_linear是:将G个embedding排成矩阵E 维度为G * M(embedding维度),再取D1个不同的矩阵和E分别作对应元素相乘再求和,此时得到D1个向量(因为每次求和后都是一个标量,共有D1个).这个向量就是y_linear.  
&emsp;&emsp;  内外积运算有一些优化方法可以降低复杂度.思路和FM中的类似,就是矩阵分解.  
&emsp;&emsp;  还有一种简化版的理解方式是,內积形态就是embedding两两作內积,如此拿到G * G长的向量(元素为a_ij),再送入一个MLP.外积形式就是embedding两两作<embedding_i,embedding_j^T>得到的是G * G个矩阵,把这些矩阵求和后送入MLP.其实简化版思想是一样的,只不过参数设置方面会有差别.  

7.DCN(Deep & Cross Network)  (内容总结自original paper)  
&emsp;&emsp;1.嵌入和堆叠   
&emsp;&emsp;&emsp;&emsp;特征切分为稠密(dense)特征x_dense和稀疏(sparse)特征x_sparse,每个x_sparse似乎由若干x_i相加得到,x_i是一系列one-hot向量.使用 W_i * x_i 拿到x_i的embedding,x_embedding_i.将所有embedding和x_dense作stack,  
&emsp;&emsp;&emsp;&emsp; x_0 = [x_embedding_1,x_embedding_2,...,x_embedding_k,x_dense]  
&emsp;&emsp;2.交叉网络(Cross Network)  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=x_{l&plus;1}&space;=&space;x_0x_l^Tw_l&plus;b_l&plus;x_l=f(x_l,w_l,b_l)&plus;x_l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{l&plus;1}&space;=&space;x_0x_l^Tw_l&plus;b_l&plus;x_l=f(x_l,w_l,b_l)&plus;x_l" title="x_{l+1} = x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l)+x_l" /></a>  
&emsp;&emsp;&emsp;&emsp;x_0即1.嵌入和堆叠拿到的.x_l与x_{l+1}为第l和l+1层输出的列向量(不妨假设共G层l=0,...G-1),注意,前面x_0x_l^Tw_l而不是x_lx_l^Tw_l,就是为了在第i层可以获取所有特征i阶交叉的可能组合,每次向交叉集中填入一个最初特征x_0.另外,x_0x_l有one-rank,有相应的快速计算和存储的方法.后面+x_l是因为要保留之前的低阶交叉,不然只留下了第i阶交叉.这部分最终输出一个向量记作x_cross.  
&emsp;&emsp;3.深度网络(Deep Network)  
&emsp;&emsp;&emsp;&emsp;  将x_0送入一个MLP,可以有很多layer.文中激活函数使用的是ReLU.这部分最终输出一个向量记作x_deep.  
&emsp;&emsp;4.融合层(Combination Layer)  
&emsp;&emsp;&emsp;&emsp;  stack交叉层和深度层的输出结果并送入MLP(文中这个MLP只有一层).  
  
8.Wide & Deep  
&emsp;&emsp;Wide部分  
&emsp;&emsp;&emsp;&emsp;Wide部分采用的是逻辑回归(logistic regression, LR),LR的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable),注意LR是一个linear model.LR的特征往往是二值且稀疏的(binary and sparse),这里采用one-hot编码.通过对稀疏的特征采取cross-product transformation(表述如下),  
&emsp;&emsp;&emsp;&emsp;cross-product transformation <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" title="\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" /></a>  
其中,c_ki是一个布尔变量,如果第i个特征是第k个transformation φk的一部分,那么值就为1，否则为0.transformation φk由人工设计,包含一系列特征,假如数据集中没出现φk的对应数据(φk即某个特征组合),那么此时对没出现的情况就缺乏泛化能力.x_i是x的第i个分量.transformation可以增加线性模型的非线性特性,从而提升模型的泛化能力.输出结果记作y_wide=[x,φ(x)],其中,φ(x)为一个向量,每个分量为φk(x).      
&emsp;&emsp;Deep部分  
&emsp;&emsp;&emsp;&emsp;Deep部分一般就是MLP,激活函数常选择ReLU.输入特征按照dense or sparse分别处理,sparse要进行embedding.将处理后的特征输入MLP,既然是Deep往往层数是比较深的.输出结果记作y_deep.  
&emsp;&emsp;联合训练(Joint Training),区别于Ensemble(集成训练,分类器是分开训练,只对结果进行汇总融合),联合训练是训练即在一起训练,同时优化参数.  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]&plus;w_{deep}^Ta^{(l_f)}&plus;b)\\=\sigma(w_{wide}^T*x_{wide}&plus;w_{deep}^T*x_{deep}&plus;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]&plus;w_{deep}^Ta^{(l_f)}&plus;b)\\=\sigma(w_{wide}^T*x_{wide}&plus;w_{deep}^T*x_{deep}&plus;b)" title="P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]+w_{deep}^Ta^{(l_f)}+b)\\=\sigma(w_{wide}^T*x_{wide}+w_{deep}^T*x_{deep}+b)" /></a>  
&emsp;&emsp;&emsp;&emsp;a^{(l_f)}即deep部分MLP最后第f层的输出向量.最终的损失函数是LR(logistic regression的损失函数,即MLE).w_wide和w_deep为输出结果的权重向量.
&emsp;&emsp;&emsp;&emsp;小技巧from某博文.Vocabularies:将类别特征(categorical features)映射为整型的id,连续的实值先用累计分布函数CDF归一化到[0,1]，再划档离散化.  
&emsp;&emsp;&emsp;&emsp;小技巧from某博文.热启动(warm-starting)方式，也就是从之前的模型中读取 embeddings 以及 linear model weights 来初始化一个新模型，而不是全部推倒重新训练.  
  
<img src="http://latex.codecogs.com/gif.latex?在此处插入公式">
9.DIN(Deep Interest Network 深度兴趣网络)From Paper      
&emsp;&emsp;1.特征表示      
&emsp;&emsp;&emsp;&emsp;第i个group feature(不知道为什么用的是group而不是field)的encoding vector表示为<img src="http://latex.codecogs.com/gif.latex?t_i\inR^{K_i},t_i[j]\in\{0,1\},\sum_{j=1}^{K_i}t_i[j]=k.">
&emsp;&emsp;2. k=1意味着t_i是one-hot encoding,k>1是multi-hot encoding.特征表示向量x=[t_1^T,t_2^T,...,t_M^T]^T,显然t_i为列向量,\sum_{i=1}^MK_i=K,K_i为第i个group特征的encoding维数,K\text{为整个特征空间的维度}.,  
&emsp;&emsp;3.  
&emsp;&emsp;4.  
  


0.标签匹配(此时就不得不介绍下本人的原创方法)  
&emsp;&emsp;1.给item打上标签,此时每个类(这个类似乎就是算法中通常说到的field领域)下的标签都会有一个经验分布(由频数或者分数归一化得到).具体哪些标签算作一类要由规则确定.假如是类为行业关键词,那么这些词就构成了这个类下的标签.
&emsp;&emsp;2.搜集用户person在item上的信息(即将每个用户在过去某个时间段内浏览的item的标签分类汇总,拿到每个类下标签的经验分布)  
&emsp;&emsp;3.依据person的各类下的概率分布和item的各类下的概率分布,按item标签的来源基于不同权重,各类予以不同权重,类内采用命中标签的概率值相乘的方式获取分数,再将该分数和类与来源的权重层层加权,最后将总分数排序,作为推荐候选名单.  
&emsp;&emsp;4.也可以将person和item的概率分布作为两个词向量(如果不只包含词就是特征向量)计算余弦相似度.  
&emsp;&emsp;5.也可以将person和item的概率分布计算交叉熵作为分数进行加权.  
&emsp;&emsp;6.也可以将person和item的概率分布作为特征传入分类算法.  





