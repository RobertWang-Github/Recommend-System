&emsp;&emsp;简单来说,推荐是个分类问题,待推荐的对象item和人person之间构成特征二元组(item_i,person_i),而click与否则是标签(c_i).  
然而,如果考虑到click和order的差异,我们需要最大化  
<a href="https://www.codecogs.com/eqnedit.php?latex=income&space;=&space;P(click|impression)*P(order|click)*price" target="_blank"><img src="https://latex.codecogs.com/gif.latex?income&space;=&space;P(click|impression)*P(order|click)*price" title="income = P(click|impression)*P(order|click)*price" /></a>  
impression(曝光)->click(点击)->order(实现) * price(价格) 才是我们需要的收益.  
&emsp;&emsp;另外,值得注意的一点是,推荐系统的可能存在对算力的一定要求.这使得我们没法直接对所有的对象item和人person执行推荐计算.只能从所有的item中先筛选出一批候选item,有时甚至要把person也筛掉一部分.这样不仅使得算力和存储占用更低,也使得结果可能更好.这就是有人说到的检索(retrieval) 的方法,对大数据集进行初步筛选,返回最匹配query的一部分物品列表.这里的检索通常会结合采用机器学习模型(machine-learned models)和人工定义规则(human-defined rules)两种方法。从大规模样本中召回最佳候选集之后,再使用排序系统对每个物品进行算分、排序,分数P(y|x),y 是用户采取的行动(比如说参加会议行为),x是特征.   
&emsp;&emsp;像Alibaba使用collaborative filtering协同过滤(基于item)实现初步筛选,拿到候选集.尤其在商品推荐上,这个策略可能相当好用,因为比如某人person点击了一个物品item A,但是没有购买,这说明他当前的需求是买和item A类似的物品,此时将所有和item A类似的物品展示给他,很可能将click行为转化成order实现价值.由此可见,用户的近期行为是十分重要的,这暗含了用户的兴趣和最近的order倾向.能否充分利用近期信息成为推荐系统能否成功的关键.  


下面对几个的推荐系统作总结:  


1.FM(Factorization Machines 因子分解机)  
&emsp;&emsp;1.线性部分(将各特征线性加权,显然线性部分直接用sigmoid激活就是LR)  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_linear&space;=&space;\sigma(<\overrightarrow{w},\overrightarrow{x}>))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_linear&space;=&space;<\overrightarrow{w},\overrightarrow{x}>" title="y_linear = <\overrightarrow{w},\overrightarrow{x}>" /></a>  
&emsp;&emsp;2.二阶部分(各特征两两组合(此处以乘积形式))  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^{(2)}*\overrightarrow{x}" /></a>  
&emsp;&emsp;&emsp;&emsp;<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>是一个n阶对称阵,因为任何两个特征x_i和x_j的关系是对称的.  
&emsp;&emsp;&emsp;&emsp;那么,<img src="https://latex.codecogs.com/gif.latex?W^{(2)}" title="W^{(2)}" /></a>可以分解为W^T * W.W为(n,k)二阶张量.原函数形式就变成了  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-polynomial}&space;=&space;\overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" title="y_{2d-polynomial} = \overrightarrow{x}^T*W^T*W*\overrightarrow{x}=<W*\overrightarrow{x},W*\overrightarrow{x}>" /></a>  
<a href="https://www.codecogs.com/eqnedit.php?latex=W*\overrightarrow{x}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W*\overrightarrow{x}" title="W*\overrightarrow{x}" /></a>可以看做是将向量x映射到低维(k维度)空间,因为W的每个行向量此时为一个基.如此,如果x是个高维稀疏向量,就被映射到了低维空间,此时再计算两两关系就会节省计算量.显然,这个低维空间是被训练的,以拿到最适合的向量表示,即embedding.向量x经过一个放射变换成为了另一个向量.这个放射变换的参数是训练参数.  
&emsp;&emsp;&emsp;&emsp;计算上来说,W*\overrightarrow{x}可以看作W的列向量按照x的分量线性加权.那么原式计算可以优化为  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" title="y_{2d-ploynomial}=\sum_{i=1}^{n}\sum_{j=1}^{n}<\overrightarrow{v_i},\overrightarrow{v_j}>x_i*x_j" /></a>  
&emsp;&emsp;&emsp;&emsp;其中,v_i和v_j皆为W的列向量.  这个优化方式只要涉及到embedding就可以使用.

&emsp;&emsp;3.输出部分(采用sigmoid作为激活函数)  
&emsp;&emsp;&emsp;&emsp;  将线性部分和二阶部分直接相加,再用激活函数作用即可.  

&emsp;&emsp;4.衍生方案(多领域FM求和)(还是注重二阶特征组合)  
&emsp;&emsp;&emsp;&emsp; FM中只有一个领域存在时,向量x的嵌入向量W * x自己和自己作內积,实现二阶组合.但是,当存在不同领域field时,可以采取将x切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding两两作內积,再将所有內积结果和线性部分结果求和送入激活函数.  

&emsp;&emsp;5.衍生方案(多领域FM拼接.embedding+MLP)(能体现特征高阶组合)
&emsp;&emsp;&emsp;&emsp;  对二阶部分的处理还可以这样做:将向量x按领域切分成L个slice,每个slice(共L个)分别经过L个放射变换,拿到L个embedding,然后将这L个embedding拼接(concat)成一个向量,送入MLP(多层感知器,即激活函数(仿射变换)->激活函数(仿射变换)->...多个串联),这样最终得到一个向量,执行一个线性加权和一个sigmoid激活就拿到了最终结果.  
&emsp;&emsp;&emsp;&emsp; embedding+MLP的落地方案有很多.FNN就是其中一个,用FM预训练这L个放射变换(FM更浅且有线性部分),且嵌入后各领域维度相同.另外,FNN似乎没有和FM一样的线性部分,所以对低阶特征组合的考虑不足,仅体现在预训练阶段.


2.FFM  
&emsp;&emsp;就是FM中的衍生方案4.在FM中引入field概念.



3.DeepFM  
&emsp;&emsp;特点1: 不需要预训练  
&emsp;&emsp;特点2: 有FM部分  
&emsp;&emsp;特点3: 有MLP部分  
&emsp;&emsp;结构：wide(领域足够多) deep(MLP足够深) -> 一阶,二阶,高阶特征都有体现.  
&emsp;&emsp;sigmoid(y_linear(不区分领域直接加权求和) + y_2d-polynomial(多领域分别embedding,两两作內积,求和) + y_MLP(多领域分别embedding,拼接后送入MLP))  

4.NFM(神经因子分解机)  
&emsp;&emsp;特点1:主要是对FM中二阶项的产生方式进行了改进.FM中的二阶项直接由不同领域的Embedding两两做內积得到.NFM修改为不同领域的Embedding两两作Hadamard product(向量对应元素相乘,得到的是一个向量),再将得到的向量求和(仍然是一个向量,长度和embedding保持一致)送入MLP,最后经过线性加权进入激活函数.将Embedding求內积的过程拆成了作Hadamard product和求和,并推迟了求和的过程.  
&emsp;&emsp;特点2:当MLP中的仿射变换和激活函数为恒等变换时,NFM退化成FM.NFM仍然可以加上y_linear部分.  

5.AFM(Attentional Factorization Machine 注意力FM)  
&emsp;&emsp;和NFM类似,都是为了对FM中二阶项进行改进.和NFM不同之处有2点.  
&emsp;&emsp;特点1:与NFM相比,没有MLP的部分,直接线性加权求和送入sigmoid
&emsp;&emsp;特点2:与NFM相比,在不同领域(设共G个)Embedding两两作Hadamard product之后(得到a_ij表示embedding i和j作Hadamard product),并不是马上将这些向量求和,而是将a_ij送入一个Attention Net, Attention Net就是 将a_ij作一个仿射变换+bias送入ReLu,在将各分量线性加权(权重系数为h)得到一个标量.C_G^2个这样的标量送入softmax(exp(a_ij)/sum(exp(a_ij))),拿到每个a_ij的权重w_ij,再计算sum(a_ij * w_ij)对所有的i,j.简而言之,就是在NFM向量求和部分加上权重了.  

6.PNN(Product-based Neural Network 基于乘积的NN)  
&emsp;&emsp;  有两种形态,內积(IPNN)与外积(OPNN).这两个形态主要是针对y_nonlinear的.不妨设field数为G
&emsp;&emsp;  內积形态:那么不同的field(将向量x切片成G份,再通过仿射变换)会得到不同的embedding共计G个,两两做內积共有G * G个元素a_ij,作为矩阵A的元素.再取D1个不同的矩阵和A分别作对应元素相乘再求和,此时得到D1个向量(因为每次求和后都是一个标量,共有D1个).这个向量就是y_nonlinear.
&emsp;&emsp;  外积形态: 此时a_ij不是由內积得到的标量,而是由embedding_i和embedding_j作 <embedding_i,embedding_j^T>得到的是一个矩阵.A由所有a_ij求和得到.其余部分一致. 
&emsp;&emsp;  此处y_linear是:将G个embedding排成矩阵E 维度为G * M(embedding维度),再取D1个不同的矩阵和E分别作对应元素相乘再求和,此时得到D1个向量(因为每次求和后都是一个标量,共有D1个).这个向量就是y_linear.  
&emsp;&emsp;  内外积运算有一些优化方法可以降低复杂度.思路和FM中的类似,就是矩阵分解.  
&emsp;&emsp;  还有一种简化版的理解方式是,內积形态就是embedding两两作內积,如此拿到G * G长的向量(元素为a_ij),再送入一个MLP.外积形式就是embedding两两作<embedding_i,embedding_j^T>得到的是G * G个矩阵,把这些矩阵求和后送入MLP.其实简化版思想是一样的,只不过参数设置方面会有差别.  

7.DCN(Deep & Cross Network)  (内容总结自original paper)  
&emsp;&emsp;1.嵌入和堆叠   
&emsp;&emsp;&emsp;&emsp;特征切分为稠密(dense)特征x_dense和稀疏(sparse)特征x_sparse,每个x_sparse似乎由若干x_i相加得到,x_i是一系列one-hot向量.使用 W_i * x_i 拿到x_i的embedding,x_embedding_i.将所有embedding和x_dense作stack,  
&emsp;&emsp;&emsp;&emsp; x_0 = [x_embedding_1,x_embedding_2,...,x_embedding_k,x_dense]  
&emsp;&emsp;2.交叉网络(Cross Network)  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=x_{l&plus;1}&space;=&space;x_0x_l^Tw_l&plus;b_l&plus;x_l=f(x_l,w_l,b_l)&plus;x_l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{l&plus;1}&space;=&space;x_0x_l^Tw_l&plus;b_l&plus;x_l=f(x_l,w_l,b_l)&plus;x_l" title="x_{l+1} = x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l)+x_l" /></a>  
&emsp;&emsp;&emsp;&emsp;x_0即1.嵌入和堆叠拿到的.x_l与x_{l+1}为第l和l+1层输出的列向量(不妨假设共G层l=0,...G-1),注意,前面x_0x_l^Tw_l而不是x_lx_l^Tw_l,就是为了在第i层可以获取所有特征i阶交叉的可能组合,每次向交叉集中填入一个最初特征x_0.另外,x_0x_l有one-rank,有相应的快速计算和存储的方法.后面+x_l是因为要保留之前的低阶交叉,不然只留下了第i阶交叉.这部分最终输出一个向量记作x_cross.  
&emsp;&emsp;3.深度网络(Deep Network)  
&emsp;&emsp;&emsp;&emsp;  将x_0送入一个MLP,可以有很多layer.文中激活函数使用的是ReLU.这部分最终输出一个向量记作x_deep.  
&emsp;&emsp;4.融合层(Combination Layer)  
&emsp;&emsp;&emsp;&emsp;  stack交叉层和深度层的输出结果并送入MLP(文中这个MLP只有一层).  
  
8.Wide & Deep  
&emsp;&emsp;Wide部分  
&emsp;&emsp;&emsp;&emsp;Wide部分采用的是逻辑回归(logistic regression, LR),LR的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable),注意LR是一个linear model.LR的特征往往是二值且稀疏的(binary and sparse),这里采用one-hot编码.通过对稀疏的特征采取cross-product transformation(表述如下),  
&emsp;&emsp;&emsp;&emsp;cross-product transformation <a href="https://www.codecogs.com/eqnedit.php?latex=\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" title="\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}}" /></a>  
其中,c_ki是一个布尔变量,如果第i个特征是第k个transformation φk的一部分,那么值就为1，否则为0.transformation φk由人工设计,包含一系列特征,假如数据集中没出现φk的对应数据(φk即某个特征组合),那么此时对没出现的情况就缺乏泛化能力.x_i是x的第i个分量.transformation可以增加线性模型的非线性特性,从而提升模型的泛化能力.输出结果记作y_wide=[x,φ(x)],其中,φ(x)为一个向量,每个分量为φk(x).      
&emsp;&emsp;Deep部分  
&emsp;&emsp;&emsp;&emsp;Deep部分一般就是MLP,激活函数常选择ReLU.输入特征按照dense or sparse分别处理,sparse要进行embedding.将处理后的特征输入MLP,既然是Deep往往层数是比较深的.输出结果记作y_deep.  
&emsp;&emsp;联合训练(Joint Training),区别于Ensemble(集成训练,分类器是分开训练,只对结果进行汇总融合),联合训练是训练即在一起训练,同时优化参数.  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]&plus;w_{deep}^Ta^{(l_f)}&plus;b)\\=\sigma(w_{wide}^T*x_{wide}&plus;w_{deep}^T*x_{deep}&plus;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]&plus;w_{deep}^Ta^{(l_f)}&plus;b)\\=\sigma(w_{wide}^T*x_{wide}&plus;w_{deep}^T*x_{deep}&plus;b)" title="P(Y=1|x)=\sigma(w_{wide}^T[x,\phi(x)]+w_{deep}^Ta^{(l_f)}+b)\\=\sigma(w_{wide}^T*x_{wide}+w_{deep}^T*x_{deep}+b)" /></a>  
&emsp;&emsp;&emsp;&emsp;a^{(l_f)}即deep部分MLP最后第f层的输出向量.最终的损失函数是LR(logistic regression的损失函数,即MLE).w_wide和w_deep为输出结果的权重向量.
&emsp;&emsp;&emsp;&emsp;小技巧from某博文.Vocabularies:将类别特征(categorical features)映射为整型的id,连续的实值先用累计分布函数CDF归一化到[0,1]，再划档离散化.  
&emsp;&emsp;&emsp;&emsp;小技巧from某博文.热启动(warm-starting)方式，也就是从之前的模型中读取 embeddings 以及 linear model weights 来初始化一个新模型，而不是全部推倒重新训练.  
  

9.DIN(Deep Interest Network 深度兴趣网络)From Paper\           
&emsp;&emsp;1.特征表示            
&emsp;&emsp;&emsp;&emsp;第i个group feature(不知道为什么用的是group而不是field)的encoding vector表示为\
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<img src="http://latex.codecogs.com/gif.latex?t_i\inR^{K_i},t_i[j]\in\{0,1\},\sum_{j=1}^{K_i}t_i[j]=k.">\
&emsp;&emsp;&emsp;&emsp;k=1意味着t_i是one-hot encoding,k>1是multi-hot encoding.特征表示向量<img src="http://latex.codecogs.com/gif.latex?x=[t_1^T,t_2^T,...,t_M^T]^T">,显然t_i为列向量,<img src="http://latex.codecogs.com/gif.latex?\sum_{i=1}^MK_i=K">,K_i为第i个group特征的encoding维数,K为整个特征空间的维度.\

&emsp;&emsp;2.Base Model(Embedding & MLP) \
&emsp;&emsp;&emsp;&emsp;和一般的Embedding一样,<img src="http://latex.codecogs.com/gif.latex?W_i=[w_1^i,...,w_{K_i}^i]\inR^{D*K_i}">,W_i * x_i就是嵌入向量embedding vector.w_{K_i}^i为每个特征feature(假如为one-hot)对应的嵌入向量.注意,当x_i为multi-hot encoding时,W_i * x_i就是将若干构成x_i的one-hot encoding分别embedding的结果加和(此时得到的结果是固定长度=D的vector).将W_i * x_i送入MLP.MLP的损失函数就是交叉熵.  

&emsp;&emsp;3.DIN的结构  \
&emsp;&emsp;&emsp;&emsp;Base Model的缺陷在于W * x是由x中所有one-hot作embedding后等权(weights=1)加和拿到的固定长度的向量,这可能导致在刻画用户person的兴趣时不够充分.但是盲目扩大embedding vector的维数(就是增大W的行维数D,这是种常见办法)会增加神经网络参数个数,增加计算和存储负担,而且还容易过拟合.DIN的更新之处在于改进刻画用户兴趣的方式上.在众多的用户历史数据中,与用户person当前点击行为有关的历史行为是更为重要的,所以要给予更高的权重,即要根据不同的候选item去关联不同的历史行为(和不同group的特征关联性不同),来给出不同权重,这就有了attention的意思.\
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=v_U(A)=f(v_A,e_1,e_2,...,e_H)=\sum_{j=1}^Ha(ej,v_A)e_j=\sum_{j=1}^Hw_je_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_U(A)=f(v_A,e_1,e_2,...,e_H)=\sum_{j=1}^Ha(ej,v_A)e_j=\sum_{j=1}^Hw_je_j" title="v_U(A)=f(v_A,e_1,e_2,...,e_H)=\sum_{j=1}^Ha(ej,v_A)e_j=\sum_{j=1}^Hw_je_j" /></a>  
&emsp;&emsp;&emsp;&emsp;{e_1,e_2,e_3,...,e_H}是用户行为的embedding vector,共有H个,v_A是itemA的嵌入向量.函数a()是一个前向神经网络(用户历史浏览的item hist和待推荐item candi的embedding作out product即hist的元素和candi的元素任意两两作乘积,拿到一个向量在和hist和candi的embedding concat起来,送入PReLU,再送入一个linear变换,就拿到了权重),输出值就是e_j的权重了(这个思路就是attention).值得注意的是,不同于传统的(attention)方法,此处的权值之和Σw_i不一定等于1,原文说权重w_i某种程度上反映了用户兴趣的强度,所以不需要归一化(保持比例难道不可以吗,这个地方感觉解释的很牵强,不过如果说值本身就很重要,在神经网络里往往如此,那就另当别论了).序列化建模,比如采取LSTM文中说没取得更好效果,因为用户历史行为序列和基于grammar的NLP任务有天然差别,在于用户的兴趣可能会来回突然切换,因为用户的兴趣经常同时存在多个,这就使得行为序列看起来充满噪音(实际上就是这样的,很多时候用户的行为前后唯一的逻辑就是都感兴趣,而不是因为看了A还要再看B).但是,从历史行为序列角度来看,某种特定的规律很可能是存在的,这个文中说要进一步研究.  

&emsp;&emsp;4.训练技术之Mini-batch Aware Regularization  
&emsp;&emsp;&emsp;&emsp;假如(goods就是items)goods_ids(包括某用户访问过的goods和所有goods)有6亿维(我去)的特征,用户的特征有6千万.如果不加入正则化项,第一个epoch之后模型明显陷入过拟合.但是因为输入特征都是稀疏的,维度又很高,网络参数又很多,直接用传统的l1,l2范数是不切实际的(因为如果没有正则化,SGD(随机梯度法)更新参数时,对于每个mini-batch只需要更新一部分网络参数,就是对应有非0输入的那些特征的参数,但是如果用l2正则化,就要最起码先把所有的参数算一遍l2范数,如果mini-batch特别多,那这个计算量就要翻很多很多倍).文中提出了一个新的计算l2-norm的方法.  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=L_2(W)=||W||_2^2=\sum_{j=1}^K||w_j||_2^2=\sum_{(x,y)\in&space;S}\sum_{j=1}^K\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\\&space;=\sum_j^K\sum_{m=1}^{B}\sum_{(x,y)\in&space;S}\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\approx\sum_{j=1}^K\sum_{m=1}^{B}\frac{\alpha_{mj}}{n_j}||w_j||_2^2\\&space;\\&space;\alpha_{mj}=max_{(x,y)\in&space;B_m}I_(x_j\ne0)&space;w_j\in&space;R^Dis\&space;the\&space;j-th\&space;embedding\&space;vector." target="_blank"><img src="https://latex.codecogs.com/gif.latex?L_2(W)=||W||_2^2=\sum_{j=1}^K||w_j||_2^2=\sum_{(x,y)\in&space;S}\sum_{j=1}^K\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\\&space;=\sum_j^K\sum_{m=1}^{B}\sum_{(x,y)\in&space;S}\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\approx\sum_{j=1}^K\sum_{m=1}^{B}\frac{\alpha_{mj}}{n_j}||w_j||_2^2\\&space;\\&space;\alpha_{mj}=max_{(x,y)\in&space;B_m}I_(x_j\ne0)&space;w_j\in&space;R^Dis\&space;the\&space;j-th\&space;embedding\&space;vector." title="L_2(W)=||W||_2^2=\sum_{j=1}^K||w_j||_2^2=\sum_{(x,y)\in S}\sum_{j=1}^K\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\\ =\sum_j^K\sum_{m=1}^{B}\sum_{(x,y)\in S}\frac{I(x_j\ne0)}{n_j}||w_j||_2^2\approx\sum_{j=1}^K\sum_{m=1}^{B}\frac{\alpha_{mj}}{n_j}||w_j||_2^2\\ \\ \alpha_{mj}=max_{(x,y)\in B_m}I_(x_j\ne0) w_j\in R^Dis\ the\ j-th\ embedding\ vector." /></a>  
&emsp;&emsp;&emsp;&emsp;I就是示性函数.对于S中的样本(x,y),只要x_j!=0就计算一次w_j的l2-norm.n_j表示特征j在所有样本里出现的次数.这个I(x_j!=0)是不大好统计的,因为要遍历mini-batch中所有样本,但是α_mj是好算的,因为在mini-batch中遇到一个样本有这个特征马上就可以去搜索下一个特征.总的来说,就是将l2-norm的计算按照特征出现与否分散(比如10/10=2/10+3/10+1/10+4/10)在许多min-batch中,权重由这个特征出现的次数决定.根据这个方式可以拿到梯度迭代更新的公式(就是求个导).  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=w_j\leftarrow&space;w_j-\eta&space;[\frac{1}{B_m}\sum_{(x,y)\in&space;B_m}\frac{\partial{L(p(x),y)}}{\partial{w_j}}&plus;\lambda\frac{\alpha_{mj}}{n_j}w_j]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_j\leftarrow&space;w_j-\eta&space;[\frac{1}{B_m}\sum_{(x,y)\in&space;B_m}\frac{\partial{L(p(x),y)}}{\partial{w_j}}&plus;\lambda\frac{\alpha_{mj}}{n_j}w_j]" title="w_j\leftarrow w_j-\eta [\frac{1}{B_m}\sum_{(x,y)\in B_m}\frac{\partial{L(p(x),y)}}{\partial{w_j}}+\lambda\frac{\alpha_{mj}}{n_j}w_j]" /></a>    
&emsp;&emsp;5.训练技术之Data Adaptive Activation Function(文中提出了著名的激活函数Dice)  
&emsp;&emsp;&emsp;&emsp;PReLU是一个很常用的激活函数.<a href="https://www.codecogs.com/eqnedit.php?latex=f(s)=p(s)*s&plus;(1-p(s))*\alpha&space;s&space;\qquad\&space;p(s)=I(s>0)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(s)=p(s)*s&plus;(1-p(s))*\alpha&space;s&space;\qquad\&space;p(s)=I(s>0)" title="f(s)=p(s)*s+(1-p(s))*\alpha s \qquad\ p(s)=I(s>0)" /></a>  
&emsp;&emsp;&emsp;&emsp;α是一个待学习的参数.PReLU在0处不连续.文中说如果神经网络不同层之间满足不同的分布,那么这个函数可能就不太适合(至少分布的均值不一定都是0,方差也未必一样,都以0作为中间点就不太合适).所以提出了Dice,函数形式如下：  
&emsp;&emsp;&emsp;&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=f(s)=p(s)*s&plus;(1-p(s))*\alpha&space;s&space;\qquad\&space;p(s)=\frac{1}{1&plus;e^{-\frac{s-E[s]}{\sqrt{Var[s]&plus;\epsilon}}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(s)=p(s)*s&plus;(1-p(s))*\alpha&space;s&space;\qquad\&space;p(s)=\frac{1}{1&plus;e^{-\frac{s-E[s]}{\sqrt{Var[s]&plus;\epsilon}}}}" title="f(s)=p(s)*s+(1-p(s))*\alpha s \qquad\ p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}" /></a>  
&emsp;&emsp;&emsp;&emsp;注意p(s)是个连续函数.将中间点平移到了E(s),而且将分布的方差规范为1.即权重方面是对信号标准化后送入sigmoid的,是unbiased.   

10.collaborative filtering(协同过滤)\
&emsp;&emsp;基本思想\

&emsp;&emsp;


0.标签匹配(此时就不得不介绍下本人的原创方法)  
&emsp;&emsp;1.给item打上标签,此时每个类(这个类似乎就是算法中通常说到的field领域)下的标签都会有一个经验分布(由频数或者分数归一化得到).具体哪些标签算作一类要由规则确定.假如是类为行业关键词,那么这些词就构成了这个类下的标签.
&emsp;&emsp;2.搜集用户person在item上的信息(即将每个用户在过去某个时间段内浏览的item的标签分类汇总,拿到每个类下标签的经验分布)  
&emsp;&emsp;3.依据person的各类下的概率分布和item的各类下的概率分布,按item标签的来源基于不同权重,各类予以不同权重,类内采用命中标签的概率值相乘的方式获取分数,再将该分数和类与来源的权重层层加权,最后将总分数排序,作为推荐候选名单.  
&emsp;&emsp;4.也可以将person和item的概率分布作为两个词向量(如果不只包含词就是特征向量)计算余弦相似度.  
&emsp;&emsp;5.也可以将person和item的概率分布计算交叉熵作为分数进行加权.  
&emsp;&emsp;6.也可以将person和item的概率分布作为特征传入分类算法.  





